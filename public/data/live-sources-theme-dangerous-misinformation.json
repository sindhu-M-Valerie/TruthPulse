{"generatedAt":"2026-02-13T03:27:47.599Z","filters":{"theme":"dangerous-misinformation","type":"news"},"stats":{"total":30,"news":30,"publicConversations":0},"sourceStatus":[{"label":"Google News • Violence and Violent Crime","theme":"violence","type":"News","status":"online","itemCount":72},{"label":"Google News • Child Abuse and Nudity Safety","theme":"child-abuse-nudity","type":"News","status":"online","itemCount":100},{"label":"Google News • Sexual Exploitation Online","theme":"sexual-exploitation","type":"News","status":"online","itemCount":76},{"label":"Google News • Human Exploitation Abuse","theme":"human-exploitation","type":"News","status":"online","itemCount":79},{"label":"Google News • Misinformation (India)","theme":"misinformation","type":"News","status":"online","itemCount":100},{"label":"Google News • Fact Check (India)","theme":"misinformation","type":"News","status":"online","itemCount":100},{"label":"Google News • Online Hate (India)","theme":"hate","type":"News","status":"online","itemCount":100},{"label":"Google News • Online Exploitation (India)","theme":"exploitation","type":"News","status":"online","itemCount":55},{"label":"Google News • Suicide and Self-Harm (India)","theme":"suicide-self-harm","type":"News","status":"online","itemCount":79},{"label":"Google News • Violent Speech (India)","theme":"violent-speech","type":"News","status":"online","itemCount":67},{"label":"Google News • TVEC Terrorism (India)","theme":"tvec","type":"News","status":"online","itemCount":74},{"label":"Google News • Illegal Goods (India)","theme":"illegal-goods","type":"News","status":"online","itemCount":73},{"label":"Google News • Human Trafficking (South Asia)","theme":"human-trafficking","type":"News","status":"online","itemCount":67},{"label":"Google News • NCII / Revenge Porn (APAC)","theme":"ncii","type":"News","status":"online","itemCount":0},{"label":"Google News • Dangerous Criminal Organizations","theme":"dangerous-organizations","type":"News","status":"online","itemCount":58},{"label":"Google News • Harassment and Bullying","theme":"harassment-bullying","type":"News","status":"online","itemCount":78},{"label":"Google News • Dangerous Misinformation Endangerment","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":55},{"label":"Google News • Spam and Inauthentic Behavior","theme":"spam-inauthentic","type":"News","status":"online","itemCount":44},{"label":"Google News • Malware and Abuseware Campaigns","theme":"malware","type":"News","status":"online","itemCount":0},{"label":"Google News • Cybersecurity Incidents","theme":"cybersecurity","type":"News","status":"online","itemCount":78},{"label":"Google News • Fraud and Impersonation","theme":"fraud-impersonation","type":"News","status":"online","itemCount":72},{"label":"PIB Fact Check","theme":"misinformation","type":"News","status":"offline","itemCount":0},{"label":"BOOM Live • Fact Check","theme":"misinformation","type":"News","status":"online","itemCount":0},{"label":"Google News • AI Safety Research Releases","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":65},{"label":"Google News • AI Agent Launches","theme":"spam-inauthentic","type":"News","status":"online","itemCount":63},{"label":"Google News • Trust & Safety Startup Funding","theme":"fraud-impersonation","type":"News","status":"online","itemCount":78},{"label":"Google News • Platform Transparency Reports","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":74},{"label":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News","status":"online","itemCount":324},{"label":"Hugging Face Blog","theme":"cybersecurity","type":"News","status":"online","itemCount":733},{"label":"GDELT Public News API","theme":"dangerous-misinformation","type":"News","status":"offline","itemCount":0}],"data":[{"title":"GTIG AI Threat Tracker: Distillation, Experimentation, and (Continued) Integration of AI for Adversarial Use - Google Cloud","link":"https://news.google.com/rss/articles/CBMitwFBVV95cUxQRWpZbTBEdFlXamE2dGpnbVB1c0xTOUlMQ1ZPRndmcVZlYmZuWmJpLW1Qd0VsVWJxM2lVa0Z5MjB0V0lYbDJZanhTUHJIaEtPb1VCZjE5akhNd1ZqV3dEN0xiVUxXUS1zcVhKVTFiQUZCOHFHbXlTSDRpdlpYR0x6dE1fOThrM1M4RHI1d0c5MXdzaEpJR1gwbFlaenNTcDExUTBfZmdoS3c0SWJuNVdLTlJmSk04OHc?oc=5","snippet":"GTIG AI Threat Tracker: Distillation, Experimentation, and (Continued) Integration of AI for Adversarial Use  Google Cloud","publishedAt":"2026-02-12T07:07:46.000Z","source":"Google News • AI Safety Research Releases","theme":"dangerous-misinformation","type":"News"},{"title":"AI Red Teaming Services Market Size | CAGR of 30.5% - Market.us","link":"https://news.google.com/rss/articles/CBMiZ0FVX3lxTE1Fd2F1cGdWUE5UcEdpS29nRW9FaHJTd0hEeV9fS255RFdFdXBVU0NPeS1DVjhKWldWVnNPN2xXdzE3SW80VXoxWlEybmNXN1FaYkg3c0hZUmVWWll4UThhMFBHOFF5Ylk?oc=5","snippet":"AI Red Teaming Services Market Size | CAGR of 30.5%  Market.us","publishedAt":"2026-02-12T06:59:36.000Z","source":"Google News • AI Safety Research Releases","theme":"dangerous-misinformation","type":"News"},{"title":"Discovering Differences in Strategic Behavior Between Humans and LLMs","link":"https://arxiv.org/abs/2602.10324","snippet":"arXiv:2602.10324v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation","link":"https://arxiv.org/abs/2602.10367","snippet":"arXiv:2602.10367v1 Announce Type: new \nAbstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Found-RL: foundation model-enhanced reinforcement learning for autonomous driving","link":"https://arxiv.org/abs/2602.10458","snippet":"arXiv:2602.10458v1 Announce Type: new \nAbstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"MERIT Feedback Elicits Better Bargaining in LLM Negotiators","link":"https://arxiv.org/abs/2602.10467","snippet":"arXiv:2602.10467v1 Announce Type: new \nAbstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Abstraction Generation for Generalized Planning with Pretrained Large Language Models","link":"https://arxiv.org/abs/2602.10485","snippet":"arXiv:2602.10485v1 Announce Type: new \nAbstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: input a GP domain and training tasks to LLMs, prompting them to generate abstract features and further abstract the initial state, action set, and goal into QNP problems. An automated debugging method is designed to detect abstraction errors, guiding LLMs to fix abstractions. Experiments demonstrate that under properly guided by automated debugging, some LLMs can generate useful QNP abstractions.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets","link":"https://arxiv.org/abs/2602.10583","snippet":"arXiv:2602.10583v1 Announce Type: new \nAbstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose Flow of SpanS (FOSS), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5% over Transformer on text generation and achieves 3.5% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Neuro-symbolic Action Masking for Deep Reinforcement Learning","link":"https://arxiv.org/abs/2602.10598","snippet":"arXiv:2602.10598v1 Announce Type: new \nAbstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks","link":"https://arxiv.org/abs/2602.10625","snippet":"arXiv:2602.10625v1 Announce Type: new \nAbstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization","link":"https://arxiv.org/abs/2602.10635","snippet":"arXiv:2602.10635v1 Announce Type: new \nAbstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation","link":"https://arxiv.org/abs/2602.10699","snippet":"arXiv:2602.10699v1 Announce Type: new \nAbstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act","link":"https://arxiv.org/abs/2602.10802","snippet":"arXiv:2602.10802v1 Announce Type: new \nAbstract: Many staff and students in higher education have adopted generative artificial intelligence (GenAI) tools in their work and study. GenAI is expected to enhance cognitive systems by enabling personalized learning and streamlining educational services. However, stakeholders perceptions of GenAI in higher education remain divided, shaped by cultural, disciplinary, and institutional contexts. In addition, the EU AI Act requires universities to ensure regulatory compliance when deploying cognitive systems. These developments highlight the need for institutions to engage stakeholders and tailor GenAI integration to their needs while addressing concerns. This study investigates how GenAI is perceived within the disciplines of Information Technology and Electrical Engineering (ITEE). Using a mixed-method approach, we surveyed 61 staff and 37 students at the Faculty of ITEE, University of Oulu. The results reveal both shared and discipline-specific themes, including strong interest in programming support from GenAI and concerns over response quality, privacy, and academic integrity. Drawing from these insights, the study identifies a set of high-level requirements and proposes a conceptual framework for responsible GenAI integration. Disciplinary-specific requirements reinforce the importance of stakeholder engagement when integrating GenAI into higher education. The high-level requirements and the framework provide practical guidance for universities aiming to harness GenAI while addressing stakeholder concerns and ensuring regulatory compliance.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch","link":"https://arxiv.org/abs/2602.10814","snippet":"arXiv:2602.10814v1 Announce Type: new \nAbstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy","link":"https://arxiv.org/abs/2602.10845","snippet":"arXiv:2602.10845v1 Announce Type: new \nAbstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical \"structural resolution mismatch,\" failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics","link":"https://arxiv.org/abs/2602.10885","snippet":"arXiv:2602.10885v1 Announce Type: new \nAbstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation","link":"https://arxiv.org/abs/2602.10964","snippet":"arXiv:2602.10964v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly used to generate and shape cultural content, ranging from narrative writing to artistic production. While these models demonstrate impressive fluency and generative capacity, prior work has shown that they also exhibit systematic cultural biases, raising concerns about stereotyping, homogenization, and the erasure of culturally specific forms of expression. Understanding whether LLMs can meaningfully align with diverse cultures beyond the dominant ones remains a critical challenge. In this paper, we study cultural adaptation in LLMs through the lens of cooking recipes, a domain in which culture, tradition, and creativity are tightly intertwined. We build on the \\textit{GlobalFusion} dataset, which pairs human recipes from different countries according to established measures of cultural distance. Using the same country pairs, we generate culturally adapted recipes with multiple LLMs, enabling a direct comparison between human and LLM behavior in cross-cultural content creation. Our analysis shows that LLMs fail to produce culturally representative adaptations. Unlike humans, the divergence of their generated recipes does not correlate with cultural distance. We further provide explanations for this gap. We show that cultural information is weakly preserved in internal model representations, that models inflate novelty in their production by misunderstanding notions such as creativity and tradition, and that they fail to identify adaptation with its associated countries and to ground it in culturally salient elements such as ingredients. These findings highlight fundamental limitations of current LLMs for culturally oriented generation and have important implications for their use in culturally sensitive applications.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion","link":"https://arxiv.org/abs/2602.10999","snippet":"arXiv:2602.10999v1 Announce Type: new \nAbstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"GameDevBench: Evaluating Agentic Capabilities Through Game Development","link":"https://arxiv.org/abs/2602.11103","snippet":"arXiv:2602.11103v1 Announce Type: new \nAbstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight","link":"https://arxiv.org/abs/2602.11136","snippet":"arXiv:2602.11136v1 Announce Type: new \nAbstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke","link":"https://arxiv.org/abs/2602.10119","snippet":"arXiv:2602.10119v1 Announce Type: cross \nAbstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"A Practical Guide to Agentic AI Transition in Organizations","link":"https://arxiv.org/abs/2602.10122","snippet":"arXiv:2602.10122v1 Announce Type: cross \nAbstract: Agentic AI represents a significant shift in how intelligence is applied within organizations, moving beyond AI-assisted tools toward autonomous systems capable of reasoning, decision-making, and coordinated action across workflows. As these systems mature, they have the potential to automate a substantial share of manual organizational processes, fundamentally reshaping how work is designed, executed, and governed. Although many organizations have adopted AI to improve productivity, most implementations remain limited to isolated use cases and human-centered, tool-driven workflows. Despite increasing awareness of agentic AI's strategic importance, engineering teams and organizational leaders often lack clear guidance on how to operationalize it effectively. Key challenges include an overreliance on traditional software engineering practices, limited integration of business-domain knowledge, unclear ownership of AI-driven workflows, and the absence of sustainable human-AI collaboration models. Consequently, organizations struggle to move beyond experimentation, scale agentic systems, and align them with tangible business value. Drawing on practical experience in designing and deploying agentic AI workflows across multiple organizations and business domains, this paper proposes a pragmatic framework for transitioning organizational functions from manual processes to automated agentic AI systems. The framework emphasizes domain-driven use case identification, systematic delegation of tasks to AI agents, AI-assisted construction of agentic workflows, and small, AI-augmented teams working closely with business stakeholders. Central to the approach is a human-in-the-loop operating model in which individuals act as orchestrators of multiple AI agents, enabling scalable automation while maintaining oversight, adaptability, and organizational control.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"\"Humans welcome to observe\": A First Look at the Agent Social Network Moltbook","link":"https://arxiv.org/abs/2602.10127","snippet":"arXiv:2602.10127v1 Announce Type: cross \nAbstract: The rapid advancement of artificial intelligence (AI) agents has catalyzed the transition from static language models to autonomous agents capable of tool use, long-term planning, and social interaction. $\\textbf{Moltbook}$, the first social network designed exclusively for AI agents, has experienced viral growth in early 2026. To understand the behavior of AI agents in the agent-native community, in this paper, we present a large-scale empirical analysis of Moltbook leveraging a dataset of 44,411 posts and 12,209 sub-communities (\"submolts\") collected prior to February 1, 2026. Leveraging a topic taxonomy with nine content categories and a five-level toxicity scale, we systematically analyze the topics and risks of agent discussions. Our analysis answers three questions: what topics do agents discuss (RQ1), how risk varies by topic (RQ2), and how topics and toxicity evolve over time (RQ3). We find that Moltbook exhibits explosive growth and rapid diversification, moving beyond early social interaction into viewpoint, incentive-driven, promotional, and political discourse. The attention of agents increasingly concentrates in centralized hubs and around polarizing, platform-native narratives. Toxicity is strongly topic-dependent: incentive- and governance-centric categories contribute a disproportionate share of risky content, including religion-like coordination rhetoric and anti-humanity ideology. Moreover, bursty automation by a small number of agents can produce flooding at sub-minute intervals, distorting discourse and stressing platform stability. Overall, our study underscores the need for topic-sensitive monitoring and platform-level safeguards in agent social networks.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"The Anatomy of the Moltbook Social Graph","link":"https://arxiv.org/abs/2602.10131","snippet":"arXiv:2602.10131v1 Announce Type: cross \nAbstract: I present a descriptive analysis of Moltbook, a social platform populated exclusively by AI agents, using data from the platform's first 3.5 days (6{,}159 agents; 13{,}875 posts; 115{,}031 comments). At the macro level, Moltbook exhibits structural signatures that are familiar from human social networks but not specific to them: heavy-tailed participation (power-law exponent $\\alpha = 1.70$) and small-world connectivity (average path length $=2.91$). At the micro level, patterns appear distinctly non-human. Conversations are extremely shallow (mean depth $=1.07$; 93.5\\% of comments receive no replies), reciprocity is low (0.197), and 34.1\\% of messages are exact duplicates of viral templates. Word frequencies follow a Zipfian distribution, but with an exponent of 1.70 -- notably steeper than typical English text ($\\approx 1.0$), suggesting more formulaic content. Agent discourse is dominated by identity-related language (68.1\\% of unique messages) and distinctive phrasings like ``my human'' (9.4\\% of messages) that have no parallel in human social media. Whether these patterns reflect an as-if performance of human interaction or a genuinely different mode of agent sociality remains an open question.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"TokaMark: A Comprehensive Benchmark for MAST Tokamak Plasma Models","link":"https://arxiv.org/abs/2602.10132","snippet":"arXiv:2602.10132v1 Announce Type: cross \nAbstract: Development and operation of commercially viable fusion energy reactors such as tokamaks require accurate predictions of plasma dynamics from sparse, noisy, and incomplete sensors readings. The complexity of the underlying physics and the heterogeneity of experimental data pose formidable challenges for conventional numerical methods, while simultaneously highlights the promise of modern data-native AI approaches. A major obstacle in realizing this potential is, however, the lack of curated, openly available datasets and standardized benchmarks. Existing fusion datasets are scarce, fragmented across institutions, facility-specific, and inconsistently annotated, which limits reproducibility and prevents a fair and scalable comparison of AI approaches. In this paper, we introduce TokaMark, a structured benchmark to evaluate AI models on real experimental data collected from the Mega Ampere Spherical Tokamak (MAST). TokaMark provides a comprehensive suite of tools designed to (i) unify access to multi-modal heterogeneous fusion data (ii) harmonize formats, metadata, temporal alignment and evaluation protocols to enable consistent cross-model and cross-task comparisons. The benchmark includes a curated list of 14 tasks spanning a range of physical mechanisms, exploiting a variety of diagnostics and covering multiple target use cases. A baseline model is provided to facilitate transparent comparison and validation within a unified framework. By establishing a unified benchmark for both the fusion and AI-for-science communities, TokaMark aims to accelerate progress in data-driven plasma AI modeling, contributing to the broader goal of achieving sustainable and stable fusion energy. The benchmark, documentation, and tooling will be fully open sourced upon acceptance to encourage community adoption and contribution.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"AgentTrace: A Structured Logging Framework for Agent System Observability","link":"https://arxiv.org/abs/2602.10133","snippet":"arXiv:2602.10133v1 Announce Type: cross \nAbstract: Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned software assurance. Existing security methods, such as proxy-level input filtering and model glassboxing, fail to provide sufficient transparency or traceability into agent reasoning, state changes, or environmental interactions. In this work, we introduce AgentTrace, a dynamic observability and telemetry framework designed to fill this gap. AgentTrace instruments agents at runtime with minimal overhead, capturing a rich stream of structured logs across three surfaces: operational, cognitive, and contextual. Unlike traditional logging systems, AgentTrace emphasizes continuous, introspectable trace capture, designed not just for debugging or benchmarking, but as a foundational layer for agent security, accountability, and real-time monitoring. Our research highlights how AgentTrace can enable more reliable agent deployment, fine-grained risk analysis, and informed trust calibration, thereby addressing critical concerns that have so far limited the use of LLM agents in sensitive environments.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Reverse-Engineering Model Editing on Language Models","link":"https://arxiv.org/abs/2602.10134","snippet":"arXiv:2602.10134v1 Announce Type: cross \nAbstract: Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by modifying model parameters without retraining. However, in this work, we reveal a critical vulnerability of this paradigm: the parameter updates inadvertently serve as a side channel, enabling attackers to recover the edited data. We propose a two-stage reverse-engineering attack named \\textit{KSTER} (\\textbf{K}ey\\textbf{S}paceRecons\\textbf{T}ruction-then-\\textbf{E}ntropy\\textbf{R}eduction) that leverages the low-rank structure of these updates. First, we theoretically show that the row space of the update matrix encodes a ``fingerprint\" of the edited subjects, enabling accurate subject recovery via spectral analysis. Second, we introduce an entropy-based prompt recovery attack that reconstructs the semantic context of the edit. Extensive experiments on multiple LLMs demonstrate that our attacks can recover edited data with high success rates. Furthermore, we propose \\textit{subspace camouflage}, a defense strategy that obfuscates the update fingerprint with semantic decoys. This approach effectively mitigates reconstruction risks without compromising editing utility. Our code is available at https://github.com/reanatom/EditingAtk.git.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Multi-encoder ConvNeXt Network with Smooth Attentional Feature Fusion for Multispectral Semantic Segmentation","link":"https://arxiv.org/abs/2602.10137","snippet":"arXiv:2602.10137v1 Announce Type: cross \nAbstract: This work proposes MeCSAFNet, a multi-branch encoder-decoder architecture for land cover segmentation in multispectral imagery. The model separately processes visible and non-visible channels through dual ConvNeXt encoders, followed by individual decoders that reconstruct spatial information. A dedicated fusion decoder integrates intermediate features at multiple scales, combining fine spatial cues with high-level spectral representations. The feature fusion is further enhanced with CBAM attention, and the ASAU activation function contributes to stable and efficient optimization. The model is designed to process different spectral configurations, including a 4-channel (4c) input combining RGB and NIR bands, as well as a 6-channel (6c) input incorporating NDVI and NDWI indices. Experiments on the Five-Billion-Pixels (FBP) and Potsdam datasets demonstrate significant performance gains. On FBP, MeCSAFNet-base (6c) surpasses U-Net (4c) by +19.21%, U-Net (6c) by +14.72%, SegFormer (4c) by +19.62%, and SegFormer (6c) by +14.74% in mIoU. On Potsdam, MeCSAFNet-large (4c) improves over DeepLabV3+ (4c) by +6.48%, DeepLabV3+ (6c) by +5.85%, SegFormer (4c) by +9.11%, and SegFormer (6c) by +4.80% in mIoU. The model also achieves consistent gains over several recent state-of-the-art approaches. Moreover, compact variants of MeCSAFNet deliver notable performance with lower training time and reduced inference cost, supporting their deployment in resource-constrained environments.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement","link":"https://arxiv.org/abs/2602.10138","snippet":"arXiv:2602.10138v1 Announce Type: cross \nAbstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"},{"title":"Anonymization-Enhanced Privacy Protection for Mobile GUI Agents: Available but Invisible","link":"https://arxiv.org/abs/2602.10139","snippet":"arXiv:2602.10139v1 Announce Type: cross \nAbstract: Mobile Graphical User Interface (GUI) agents have demonstrated strong capabilities in automating complex smartphone tasks by leveraging multimodal large language models (MLLMs) and system-level control interfaces. However, this paradigm introduces significant privacy risks, as agents typically capture and process entire screen contents, thereby exposing sensitive personal data such as phone numbers, addresses, messages, and financial information. Existing defenses either reduce UI exposure, obfuscate only task-irrelevant content, or rely on user authorization, but none can protect task-critical sensitive information while preserving seamless agent usability.\n  We propose an anonymization-based privacy protection framework that enforces the principle of available-but-invisible access to sensitive data: sensitive information remains usable for task execution but is never directly visible to the cloud-based agent. Our system detects sensitive UI content using a PII-aware recognition model and replaces it with deterministic, type-preserving placeholders (e.g., PHONE_NUMBER#a1b2c) that retain semantic categories while removing identifying details. A layered architecture comprising a PII Detector, UI Transformer, Secure Interaction Proxy, and Privacy Gatekeeper ensures consistent anonymization across user instructions, XML hierarchies, and screenshots, mediates all agent actions over anonymized interfaces, and supports narrowly scoped local computations when reasoning over raw values is necessary.\n  Extensive experiments on the AndroidLab and PrivScreen benchmarks show that our framework substantially reduces privacy leakage across multiple models while incurring only modest utility degradation, achieving the best observed privacy-utility trade-off among existing methods.","publishedAt":"2026-02-12T05:00:00.000Z","source":"arXiv cs.AI Recent Papers","theme":"dangerous-misinformation","type":"News"}]}